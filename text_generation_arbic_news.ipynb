{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1N7oWBSCPIyEsvKhlMfWMtK0F9sHPjRqr",
      "authorship_tag": "ABX9TyO9pzW7sNY3LA0ERqQ3x8si",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamedatef773/Data-Science/blob/main/text_generation_arbic_news.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mnto026Zkj0M"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np \n",
        "import os \n",
        "import time\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "# NGrams"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ukbvqdac2i4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Search for data\n",
        "#### Arabic-News\n",
        "##### Arabic News for language modelling collected from\n",
        "\n",
        "##### BBC Arabic \n",
        "##### EuroNews\n",
        "##### Aljazeera\n",
        "##### CNN Arabic\n",
        "##### RT Arabic"
      ],
      "metadata": {
        "id": "nNAYL53cl0dN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read the data  \n",
        "text=open('/content/drive/MyDrive/aljazeera.net_20190419_titles.txt','rb').read().decode(encoding='utf-8')\n"
      ],
      "metadata": {
        "id": "0OOSai7XtnwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def preprocess(text):  \n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return \"\".join(text)\n"
      ],
      "metadata": {
        "id": "2vFJvJs8FUP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=preprocess(text)"
      ],
      "metadata": {
        "id": "FfMOHfltF82B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:250]"
      ],
      "metadata": {
        "id": "WnC13xjvoCFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the unique characters in the file \n",
        "vocab=sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))\n",
        "vocab"
      ],
      "metadata": {
        "id": "ZCB5JlzhuzvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The i:j part of the comprehension creates a new key-value pair in the dictionary, with the key being the element j and the value being the index i.\n",
        "char_idx={j:i for i,j in enumerate(vocab)}\n",
        "idx_char=np.array(vocab)\n",
        "# convert the text to int\n",
        "text_as_int=np.array([char_idx[c] for c in text])\n"
      ],
      "metadata": {
        "id": "vTULk00rx045"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_idx"
      ],
      "metadata": {
        "id": "CNQvJitGMHKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_as_int[:255]"
      ],
      "metadata": {
        "id": "60pvFK1iyM35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show ho the first 20 chararacters from the text are mapped to integers\n",
        "print('{}--------- characters mapped to int--------> {}'.format(repr(text[:13]),text_as_int[:13]))"
      ],
      "metadata": {
        "id": "kPjGzofayN5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the maximum lenght sentence for the single input in characters  \n",
        "seq_length=100\n",
        "# create training exaples / targets\n",
        "char_dataset=tf.data.Dataset.from_tensor_slices(text_as_int)\n"
      ],
      "metadata": {
        "id": "X0sptO9ELmeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to make the input the same size in the character  and add the one character to make the  traget \n",
        "squences=char_dataset.batch(seq_length+1,drop_remainder=True)\n"
      ],
      "metadata": {
        "id": "1x_Qj5ZBNUeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "function returns a printable representation of the given object.\n"
      ],
      "metadata": {
        "id": "zYFBfao7ychc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for item in squences.take(5):\n",
        "  print(repr(''.join(idx_char[item.numpy()])),item,len(item))"
      ],
      "metadata": {
        "id": "zksK64yHxgqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text=chunk[:-1]\n",
        "  traget_text=chunk[1:]\n",
        "  return input_text,traget_text"
      ],
      "metadata": {
        "id": "6mhXA1g-xmZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kpv2dLoyAFuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=squences.map(split_input_target)"
      ],
      "metadata": {
        "id": "ahUKDLiAzmHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i ,j in dataset.take(1):\n",
        "  print('input data','',''.join(idx_char[i.numpy()]))\n",
        "  print('Taraget data','',''.join(idx_char[j.numpy()]))"
      ],
      "metadata": {
        "id": "3Uh1gbzp0h6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "id": "AfqsY1hd1mBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=64\n",
        "#  BUFFER_SIZE (which starts empty but has enough room to store that many elements). \n",
        "BUFFER_SIZE=10000\n",
        "# make the  dataset shuffle  and put in the batch\n",
        "dataset=dataset.shuffle(BUFFER_SIZE).batch(batch_size,drop_remainder=True)"
      ],
      "metadata": {
        "id": "htEhXpgD3U4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocb_size=len(vocab)\n",
        "embedding_dim=256\n",
        "ruu_units=1024"
      ],
      "metadata": {
        "id": "Q2PN1UIG3wwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocb_size,embedding_dim,ruu_units,batch_size):\n",
        "  model=tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(vocb_size,embedding_dim,batch_input_shape=[batch_size,None]),\n",
        "      tf.keras.layers.GRU(ruu_units,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'),\n",
        "      #                                                                                              Xavier/ wights initializations\n",
        "      tf.keras.layers.Dense(vocb_size)\n",
        "              ])\n",
        "  return model"
      ],
      "metadata": {
        "id": "HK1VKpk47Ak9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=build_model(vocb_size,embedding_dim,ruu_units,batch_size)"
      ],
      "metadata": {
        "id": "UiVRvGZFJ7rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "_pDQ0Ipgf1pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this crossentropy loss function when there are two or more label classes."
      ],
      "metadata": {
        "id": "0XX_8qyVhKsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'adam',loss='sparse_categorical_crossentropy')\n"
      ],
      "metadata": {
        "id": "u_-FO9kYhTkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(dataset,epochs=20)"
      ],
      "metadata": {
        "id": "9NLNaqyn-Qsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_as_int[:255]"
      ],
      "metadata": {
        "id": "jeGdrs83IcIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ySfzDuNbNC57"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}